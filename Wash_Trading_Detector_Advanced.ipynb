{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-14T13:02:03.958747Z",
     "start_time": "2025-10-14T13:02:03.956067Z"
    }
   },
   "source": [
    "# pip install networkx pandas numpy (if not already available)\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Dict, Iterable, Tuple, Set, FrozenSet, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from src.data_handler import CoinDataStore\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:02:04.129545Z",
     "start_time": "2025-10-14T13:02:04.126348Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8df5c4620686c3b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:11:57.426187Z",
     "start_time": "2025-10-14T14:11:57.398034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "from typing import Dict, Iterable, List, Tuple, Set, FrozenSet, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) SAFETY / DTYPE HELPERS\n",
    "# =========================\n",
    "\n",
    "def _ensure_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make a safe copy with the expected dtypes:\n",
    "      price: float64 (ok if originally float32)\n",
    "      size:  float64\n",
    "      time:  datetime64[ns]\n",
    "      seller, buyer: int (Python ints)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"price\"]  = df[\"price\"].astype(\"float64\")\n",
    "    df[\"size\"]   = df[\"size\"].astype(\"float64\")\n",
    "    df[\"time\"]   = pd.to_datetime(df[\"time\"], utc=False)\n",
    "    df[\"seller\"] = df[\"seller\"].astype(\"uint64\").astype(int)\n",
    "    df[\"buyer\"]  = df[\"buyer\"].astype(\"uint64\").astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 1) ALGORITHM 1 (SCC peel)\n",
    "# ========================\n",
    "\n",
    "def build_weighted_digraph_from_trades(df: pd.DataFrame) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    One edge per (seller -> buyer) with 'weight' = number of rows (trades).\n",
    "    This collapses partial fills into edge weight (count of parallel trades).\n",
    "    \"\"\"\n",
    "    df = _ensure_schema(df)\n",
    "\n",
    "    edge_counts = (\n",
    "        df.groupby([\"seller\", \"buyer\"], as_index=False)\n",
    "          .size()\n",
    "          .rename(columns={\"size\": \"weight\"})\n",
    "    )\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for row in edge_counts.itertuples(index=False):\n",
    "        G.add_edge(int(row.seller), int(row.buyer), weight=int(row.weight))\n",
    "    return G\n",
    "\n",
    "\n",
    "def iterative_scc_peeling(\n",
    "    G_in: nx.DiGraph,\n",
    "    *,\n",
    "    min_scc_size: int = 2,\n",
    "    count_singletons_with_selfloop: bool = True,\n",
    ") -> Counter[Frozenset[int]]:\n",
    "    \"\"\"\n",
    "    Iteratively:\n",
    "      - find SCCs (record their vertex sets)\n",
    "      - decrement every edge weight by 1; remove zeros\n",
    "    Stop when no edges remain. Return a Counter of {frozenset(addresses): occurrences}.\n",
    "    \"\"\"\n",
    "    G = G_in.copy(as_view=False)\n",
    "\n",
    "    # normalize weights\n",
    "    for _, _, data in G.edges(data=True):\n",
    "        w = int(data.get(\"weight\", 1))\n",
    "        if w <= 0:\n",
    "            raise ValueError(\"Edge weight must be positive.\")\n",
    "        data[\"weight\"] = w\n",
    "\n",
    "    scc_counter: Counter[Frozenset[int]] = Counter()\n",
    "\n",
    "    def should_count_scc(scc: Set[int]) -> bool:\n",
    "        if len(scc) >= max(2, min_scc_size):\n",
    "            return True\n",
    "        if len(scc) == 1 and count_singletons_with_selfloop:\n",
    "            n = next(iter(scc))\n",
    "            return G.has_edge(n, n)\n",
    "        return False\n",
    "\n",
    "    while G.number_of_edges() > 0:\n",
    "        # 1) SCCs on current (unweighted) structure\n",
    "        sccs: List[Set[int]] = list(nx.strongly_connected_components(G))\n",
    "        # 2) record vertex sets\n",
    "        for scc in sccs:\n",
    "            if should_count_scc(scc):\n",
    "                scc_counter[frozenset(scc)] += 1\n",
    "        # 3) peel one layer\n",
    "        to_remove = []\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            data[\"weight\"] -= 1\n",
    "            if data[\"weight\"] <= 0:\n",
    "                to_remove.append((u, v))\n",
    "        G.remove_edges_from(to_remove)\n",
    "\n",
    "    return scc_counter\n",
    "\n",
    "\n",
    "def algorithm1_for_token_df(\n",
    "    df_token: pd.DataFrame,\n",
    "    *,\n",
    "    min_scc_size: int = 2,\n",
    "    count_singletons_with_selfloop: bool = True,\n",
    "    candidate_threshold: int = 100,\n",
    ") -> Tuple[Counter[Frozenset[int]], List[Tuple[Frozenset[int], int]]]:\n",
    "    \"\"\"\n",
    "    Run Algorithm 1 on a single-token DataFrame.\n",
    "    Returns:\n",
    "      - scc_counter: frozenset(addresses) -> occurrence count\n",
    "      - candidates:  list[(frozenset(addresses), count)] with count >= candidate_threshold (sorted desc)\n",
    "    \"\"\"\n",
    "    G = build_weighted_digraph_from_trades(df_token)\n",
    "    scc_counter = iterative_scc_peeling(\n",
    "        G,\n",
    "        min_scc_size=min_scc_size,\n",
    "        count_singletons_with_selfloop=count_singletons_with_selfloop,\n",
    "    )\n",
    "    candidates = [(S, c) for S, c in scc_counter.items() if c >= candidate_threshold]\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scc_counter, candidates\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 2) POSITION SUMS (HELPERS)\n",
    "# ==========================\n",
    "\n",
    "@dataclass\n",
    "class PositionResult:\n",
    "    positions: pd.Series          # index: account id, values: net token position\n",
    "    v_mean: float                 # mean trade size in the set\n",
    "    v_tol: float                  # v = m * v_mean\n",
    "    max_abs_pos: float            # max_i |p_i|\n",
    "    is_wash: bool                 # True if max_abs_pos <= v_tol\n",
    "    n_trades: int\n",
    "\n",
    "\n",
    "def compute_positions(df_window: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Per-account token positions for a set of trades:\n",
    "      buyer +size, seller -size.\n",
    "    \"\"\"\n",
    "    if df_window.empty:\n",
    "        return pd.Series(dtype=\"float64\")\n",
    "\n",
    "    df_window = _ensure_schema(df_window)\n",
    "\n",
    "    pos_buy  = df_window.groupby(\"buyer\")[\"size\"].sum()\n",
    "    pos_sell = (-df_window[\"size\"]).groupby(df_window[\"seller\"]).sum()\n",
    "\n",
    "    positions = pos_buy.add(pos_sell, fill_value=0.0).astype(\"float64\")\n",
    "    return positions\n",
    "\n",
    "\n",
    "def wash_test(df_window: pd.DataFrame, m: float = 0.01) -> PositionResult:\n",
    "    \"\"\"\n",
    "    Return whether the set of trades is a 'wash result':\n",
    "      max_i |position_i| <= m * mean(size)\n",
    "    \"\"\"\n",
    "    n = len(df_window)\n",
    "    if n == 0:\n",
    "        return PositionResult(\n",
    "            positions=pd.Series(dtype=\"float64\"),\n",
    "            v_mean=0.0, v_tol=0.0, max_abs_pos=np.inf, is_wash=False, n_trades=0\n",
    "        )\n",
    "    if n == 1:\n",
    "        # Need >=2 trades for a meaningful loop\n",
    "        positions = compute_positions(df_window)\n",
    "        v_mean = float(df_window[\"size\"].mean())\n",
    "        v_tol = m * v_mean\n",
    "        max_abs_pos = float(np.abs(positions).max()) if len(positions) else 0.0\n",
    "        return PositionResult(positions, v_mean, v_tol, max_abs_pos, False, n)\n",
    "\n",
    "    v_mean = float(df_window[\"size\"].mean())\n",
    "    v_tol = m * v_mean\n",
    "    positions = compute_positions(df_window)\n",
    "    max_abs_pos = float(np.abs(positions).max()) if len(positions) else 0.0\n",
    "    is_wash = (max_abs_pos <= v_tol)\n",
    "    return PositionResult(positions, v_mean, v_tol, max_abs_pos, is_wash, n)\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# 3) ALGORITHM 2 (WINDOWED PREFIX MATCH)\n",
    "# =====================================\n",
    "\n",
    "def algorithm2_volume_matching(\n",
    "    df_token: pd.DataFrame,\n",
    "    candidate_wallets: Iterable[int],\n",
    "    *,\n",
    "    m: float = 0.01,\n",
    "    windows: Tuple[str, ...] = (\"1H\", \"1D\", \"7D\"),\n",
    "    min_trades_in_set: int = 2,\n",
    "    week_floor: Optional[str] = None,   # e.g. \"W-MON\"\n",
    "    id_prefix: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Label concrete wash trades among the candidate wallet set S within 1H -> 1D -> 7D tumbling windows.\n",
    "    - Trades labeled in a smaller window are not reconsidered in larger windows.\n",
    "    - Inside a window, scan prefixes (longest first), accept those that pass wash_test, and continue.\n",
    "    Returns a copy of df_token with label columns filled.\n",
    "    \"\"\"\n",
    "    df = _ensure_schema(df_token)\n",
    "\n",
    "    # Only trades inside the candidate set\n",
    "    S: Set[int] = set(int(x) for x in candidate_wallets)\n",
    "    df_S = df.loc[df[\"seller\"].isin(S) & df[\"buyer\"].isin(S)].sort_values(\"time\").copy()\n",
    "\n",
    "    # Prepare label columns for the whole df (others remain False/NaN)\n",
    "    df[\"is_wash\"] = False\n",
    "    for col, dtype in {\n",
    "        \"wash_id\": \"object\",\n",
    "        \"wash_window\": \"object\",\n",
    "        \"wash_window_start\": \"datetime64[ns]\",\n",
    "        \"wash_window_end\":   \"datetime64[ns]\",\n",
    "        \"wash_n_trades\": \"float64\",\n",
    "        \"wash_m\": \"float64\",\n",
    "        \"wash_v_mean\": \"float64\",\n",
    "        \"wash_v_tol\": \"float64\",\n",
    "        \"wash_max_abs_pos\": \"float64\",\n",
    "    }.items():\n",
    "        df[col] = pd.Series(index=df.index, dtype=dtype)\n",
    "\n",
    "    if df_S.empty:\n",
    "        return df\n",
    "\n",
    "    labeled_idx: Set[int] = set()\n",
    "    wash_counter = 0\n",
    "\n",
    "    def label_trades(\n",
    "        idxs: List[int],\n",
    "        window_tag: str,\n",
    "        w_start: pd.Timestamp,\n",
    "        w_end: pd.Timestamp,\n",
    "        res: PositionResult,\n",
    "    ):\n",
    "        nonlocal wash_counter\n",
    "        wash_counter += 1\n",
    "        wid = f\"{id_prefix or ''}{wash_counter}\" if id_prefix else str(wash_counter)\n",
    "\n",
    "        df.loc[idxs, \"is_wash\"] = True\n",
    "        df.loc[idxs, \"wash_id\"] = wid\n",
    "        df.loc[idxs, \"wash_window\"] = window_tag\n",
    "        df.loc[idxs, \"wash_window_start\"] = w_start\n",
    "        df.loc[idxs, \"wash_window_end\"] = w_end\n",
    "        df.loc[idxs, \"wash_n_trades\"] = res.n_trades\n",
    "        df.loc[idxs, \"wash_m\"] = m\n",
    "        df.loc[idxs, \"wash_v_mean\"] = res.v_mean\n",
    "        df.loc[idxs, \"wash_v_tol\"] = res.v_tol\n",
    "        df.loc[idxs, \"wash_max_abs_pos\"] = res.max_abs_pos\n",
    "        labeled_idx.update(idxs)\n",
    "\n",
    "    for win in windows:\n",
    "        print(f\"Window {win} is in process\")\n",
    "        # build tumbling window key\n",
    "        if week_floor and (win.upper().startswith(\"W\") or win.upper() in {\"1W\", \"7D\"}):\n",
    "            wkey = df_S[\"time\"].dt.to_period(week_floor).dt.start_time\n",
    "        else:\n",
    "            wkey = df_S[\"time\"].dt.floor(win)\n",
    "\n",
    "        df_S = df_S.assign(_wkey=wkey)\n",
    "\n",
    "        for w, g in df_S.groupby(\"_wkey\", sort=True):\n",
    "            # consider only unlabeled rows (by original indices in df)\n",
    "            idxs_all = [int(i) for i in g.index if i not in labeled_idx]\n",
    "            if len(idxs_all) < min_trades_in_set:\n",
    "                continue\n",
    "\n",
    "            # We can find multiple disjoint wash prefixes in the same window\n",
    "            remaining = idxs_all.copy()\n",
    "            while len(remaining) >= min_trades_in_set:\n",
    "                found_any = False\n",
    "\n",
    "                # Try longest prefix first, then shorten\n",
    "                for k in range(len(remaining), min_trades_in_set - 1, -1):\n",
    "                    prefix_idxs = remaining[:k]\n",
    "                    df_prefix = df.loc[prefix_idxs, [\"price\",\"size\",\"time\",\"seller\",\"buyer\"]].sort_values(\"time\")\n",
    "                    res = wash_test(df_prefix, m=m)\n",
    "                    if res.is_wash:\n",
    "                        w_start = pd.to_datetime(w)\n",
    "                        # rough end = start + window; special-case 7D/1W\n",
    "                        if isinstance(win, str) and win[0].isdigit():\n",
    "                            w_end = w_start + pd.to_timedelta(win)\n",
    "                        elif win.upper() in {\"1W\", \"7D\"}:\n",
    "                            w_end = w_start + pd.Timedelta(days=7)\n",
    "                        else:\n",
    "                            w_end = w_start + pd.Timedelta(days=7)\n",
    "\n",
    "                        label_trades(prefix_idxs, win, w_start, w_end, res)\n",
    "                        remaining = remaining[k:]  # continue after the labeled block\n",
    "                        found_any = True\n",
    "                        break\n",
    "\n",
    "                if not found_any:\n",
    "                    break\n",
    "\n",
    "        # cleanup helper\n",
    "        df_S = df_S.drop(columns=[\"_wkey\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 4) PIPELINE RUNNER(S)\n",
    "# ======================\n",
    "\n",
    "def run_full_pipeline_for_token(\n",
    "    df_token: pd.DataFrame,\n",
    "    *,\n",
    "    # Alg. 1 knobs\n",
    "    min_scc_size: int = 2,\n",
    "    count_singletons_with_selfloop: bool = True,\n",
    "    candidate_threshold: int = 100,\n",
    "    # Alg. 2 knobs\n",
    "    m: float = 0.01,\n",
    "    windows: Tuple[str, ...] = (\"1H\", \"1D\", \"7D\"),\n",
    "    min_trades_in_set: int = 2,\n",
    "    week_floor: Optional[str] = None,\n",
    "    # labeling id prefix (e.g., token name for readability)\n",
    "    id_prefix: Optional[str] = None,\n",
    ") -> Tuple[pd.DataFrame, List[Tuple[Frozenset[int], int]]]:\n",
    "    \"\"\"\n",
    "    End-to-end for ONE TOKEN:\n",
    "      - Algorithm 1 -> candidate SCC sets\n",
    "      - Algorithm 2 -> label concrete wash trades within each candidate\n",
    "    Returns:\n",
    "      - labeled DataFrame (with columns is_wash, wash_id, diagnostics, ...)\n",
    "      - the list of candidates (S, count) from Algorithm 1\n",
    "    \"\"\"\n",
    "    df_token = _ensure_schema(df_token)\n",
    "\n",
    "    # --- Algorithm 1\n",
    "    scc_counter, candidates = algorithm1_for_token_df(\n",
    "        df_token,\n",
    "        min_scc_size=min_scc_size,\n",
    "        count_singletons_with_selfloop=count_singletons_with_selfloop,\n",
    "        candidate_threshold=candidate_threshold,\n",
    "    )\n",
    "\n",
    "    # --- Algorithm 2 (apply per candidate, never double-label)\n",
    "    # start with unlabeled df; for each candidate, label its trades\n",
    "    labeled_df = df_token.copy()\n",
    "    # initialize label columns so .loc works even if no matches\n",
    "    for col in [\"is_wash\",\"wash_id\",\"wash_window\",\"wash_window_start\",\"wash_window_end\",\n",
    "                \"wash_n_trades\",\"wash_m\",\"wash_v_mean\",\"wash_v_tol\",\"wash_max_abs_pos\"]:\n",
    "        if col not in labeled_df:\n",
    "            labeled_df[col] = np.nan if col not in {\"is_wash\"} else False\n",
    "\n",
    "    for S, _count in candidates:\n",
    "        # run Algorithm 2 for this candidate set S\n",
    "        df_S_labeled = algorithm2_volume_matching(\n",
    "            labeled_df,\n",
    "            candidate_wallets=S,\n",
    "            m=m,\n",
    "            windows=windows,\n",
    "            min_trades_in_set=min_trades_in_set,\n",
    "            week_floor=week_floor,\n",
    "            id_prefix=id_prefix,\n",
    "        )\n",
    "        # merge new labels (logical OR for is_wash; overwrite diagnostics where newly labeled)\n",
    "        newly = df_S_labeled[\"is_wash\"] & ~labeled_df[\"is_wash\"]\n",
    "        for col in [\"is_wash\",\"wash_id\",\"wash_window\",\"wash_window_start\",\"wash_window_end\",\n",
    "                    \"wash_n_trades\",\"wash_m\",\"wash_v_mean\",\"wash_v_tol\",\"wash_max_abs_pos\"]:\n",
    "            labeled_df.loc[newly, col] = df_S_labeled.loc[newly, col]\n",
    "\n",
    "    return labeled_df, candidates\n",
    "\n",
    "\n",
    "def summarize_wash_stats(df_labeled: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Small helper to get a quick summary for a token:\n",
    "      - number of wash sets\n",
    "      - share of wash trades (count-based)\n",
    "      - share of wash volume (token-size-based)\n",
    "    \"\"\"\n",
    "    df = _ensure_schema(df_labeled)\n",
    "    total_trades = len(df)\n",
    "    total_vol = float(df[\"size\"].sum())\n",
    "    wash_mask = df[\"is_wash\"].fillna(False).astype(bool)\n",
    "    wash_trades = int(wash_mask.sum())\n",
    "    wash_vol = float(df.loc[wash_mask, \"size\"].sum())\n",
    "\n",
    "    # count distinct wash episodes\n",
    "    n_sets = df.loc[wash_mask, \"wash_id\"].nunique()\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"n_trades_total\": [total_trades],\n",
    "            \"n_trades_wash\": [wash_trades],\n",
    "            \"wash_trade_share_pct\": [100.0 * wash_trades / total_trades if total_trades else 0.0],\n",
    "            \"volume_total\": [total_vol],\n",
    "            \"volume_wash\": [wash_vol],\n",
    "            \"wash_volume_share_pct\": [100.0 * wash_vol / total_vol if total_vol else 0.0],\n",
    "            \"n_wash_sets\": [int(n_sets)],\n",
    "        }\n",
    "    )\n"
   ],
   "id": "9952ce2cb81de2d4",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:02:04.452154Z",
     "start_time": "2025-10-14T13:02:04.449558Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e2adfbddaf3891f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:02:05.092006Z",
     "start_time": "2025-10-14T13:02:04.858442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "store = CoinDataStore(\"AVAX\")\n",
    "df_avax = store.load_days(store.list_days()[-10:])\n",
    "df_avax"
   ],
   "id": "a0db7c45cc1c980b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            price        size                    time  seller   buyer\n",
       "0       23.941000    6.890000 2025-07-18 00:00:06.890      86      19\n",
       "1       23.990999   12.930000 2025-07-18 00:00:08.507   10169     480\n",
       "2       24.000000   42.169998 2025-07-18 00:00:09.188      22       3\n",
       "3       24.004000   16.770000 2025-07-18 00:00:09.188       8       3\n",
       "4       23.990999   42.099998 2025-07-18 00:00:10.919      22   10200\n",
       "...           ...         ...                     ...     ...     ...\n",
       "147426  24.966999    1.210000 2025-07-27 08:44:33.762       8    1527\n",
       "147427  24.971001  308.359985 2025-07-27 08:44:59.437     161    5135\n",
       "147428  24.971001  203.639999 2025-07-27 08:44:59.437     892    5135\n",
       "147429  24.971001   11.940000 2025-07-27 08:45:07.130      19  170117\n",
       "147430  24.975000    0.670000 2025-07-27 08:45:47.161    5165      19\n",
       "\n",
       "[147431 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>time</th>\n",
       "      <th>seller</th>\n",
       "      <th>buyer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.941000</td>\n",
       "      <td>6.890000</td>\n",
       "      <td>2025-07-18 00:00:06.890</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.990999</td>\n",
       "      <td>12.930000</td>\n",
       "      <td>2025-07-18 00:00:08.507</td>\n",
       "      <td>10169</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>42.169998</td>\n",
       "      <td>2025-07-18 00:00:09.188</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.004000</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>2025-07-18 00:00:09.188</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.990999</td>\n",
       "      <td>42.099998</td>\n",
       "      <td>2025-07-18 00:00:10.919</td>\n",
       "      <td>22</td>\n",
       "      <td>10200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147426</th>\n",
       "      <td>24.966999</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>2025-07-27 08:44:33.762</td>\n",
       "      <td>8</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147427</th>\n",
       "      <td>24.971001</td>\n",
       "      <td>308.359985</td>\n",
       "      <td>2025-07-27 08:44:59.437</td>\n",
       "      <td>161</td>\n",
       "      <td>5135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147428</th>\n",
       "      <td>24.971001</td>\n",
       "      <td>203.639999</td>\n",
       "      <td>2025-07-27 08:44:59.437</td>\n",
       "      <td>892</td>\n",
       "      <td>5135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147429</th>\n",
       "      <td>24.971001</td>\n",
       "      <td>11.940000</td>\n",
       "      <td>2025-07-27 08:45:07.130</td>\n",
       "      <td>19</td>\n",
       "      <td>170117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147430</th>\n",
       "      <td>24.975000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>2025-07-27 08:45:47.161</td>\n",
       "      <td>5165</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147431 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:02:07.185384Z",
     "start_time": "2025-10-14T13:02:07.182684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# store = CoinDataStore(\"AVAX\")\n",
    "# df_avax = store.load_all()\n",
    "# df_avax"
   ],
   "id": "2a00eb41ea0eea4e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:02:07.974836Z",
     "start_time": "2025-10-14T13:02:07.520054Z"
    }
   },
   "cell_type": "code",
   "source": "df_avax.to_csv(\"AVAX_small.csv\", index=False)",
   "id": "ece0f524c7b9a194",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:06:41.498100Z",
     "start_time": "2025-10-14T13:06:41.488167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "records = [\n",
    "    [\"A\", \"B\"],\n",
    "    [\"B\", \"A\"],\n",
    "    [\"A\", \"B\"],\n",
    "    [\"B\", \"A\"],\n",
    "    [\"A\", \"B\"],\n",
    "    [\"B\", \"A\"],\n",
    "    [\"A\", \"B\"],\n",
    "    [\"B\", \"A\"],\n",
    "    [\"A\", \"B\"],\n",
    "    [\"B\", \"A\"],\n",
    "\n",
    "    [\"B\", \"C\"],\n",
    "    [\"C\", \"A\"],\n",
    "\n",
    "    [\"D\", \"E\"],\n",
    "    [\"E\", \"D\"],\n",
    "]\n",
    "\n",
    "df_test = pd.DataFrame(records, columns=[\"seller\", \"buyer\"])\n",
    "df_test = df_test.reset_index().rename(columns={\"index\": \"time\"})\n",
    "df_test[\"price\"] = 10\n",
    "df_test[\"size\"] = 10\n",
    "df_test"
   ],
   "id": "d923a2047a391ee6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    time  seller  buyer  price  size\n",
       "0      0       0      1     10    10\n",
       "1      1       1      0     10    10\n",
       "2      2       0      1     10    10\n",
       "3      3       1      0     10    10\n",
       "4      4       0      1     10    10\n",
       "5      5       1      0     10    10\n",
       "6      6       0      1     10    10\n",
       "7      7       1      0     10    10\n",
       "8      8       0      1     10    10\n",
       "9      9       1      0     10    10\n",
       "10    10       1      2     10    10\n",
       "11    11       2      0     10    10\n",
       "12    12       3      4     10    10\n",
       "13    13       4      3     10    10"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>seller</th>\n",
       "      <th>buyer</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:06:42.825968Z",
     "start_time": "2025-10-14T13:06:42.822330Z"
    }
   },
   "cell_type": "code",
   "source": "df_avax = df_test",
   "id": "9cfe5ba3d2df67cd",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:07:30.967576Z",
     "start_time": "2025-10-14T13:07:30.965341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#presettings\n",
    "df_token = df_avax.copy()\n",
    "min_scc_size: int = 2\n",
    "count_singletons_with_selfloop = True\n",
    "candidate_threshold = 30"
   ],
   "id": "8a89ad20dd347413",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:07:31.416425Z",
     "start_time": "2025-10-14T13:07:31.409923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# algorithm 1\n",
    "df_token = _ensure_schema(df_token)\n",
    "\n",
    "# --- Algorithm 1\n",
    "scc_counter, candidates = algorithm1_for_token_df(\n",
    "    df_token,\n",
    "    min_scc_size=min_scc_size,\n",
    "    count_singletons_with_selfloop=count_singletons_with_selfloop,\n",
    "    candidate_threshold=candidate_threshold,\n",
    ")"
   ],
   "id": "19e62931b95775e",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:07:32.521915Z",
     "start_time": "2025-10-14T13:07:32.519276Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b1e34daf2812eb22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:07:32.890209Z",
     "start_time": "2025-10-14T13:07:32.886239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m=0.01                  # 1% tolerance for position sums\n",
    "windows=(\"1h\",\"1D\",\"7D\")     # hour -> day -> week passes\n",
    "id_prefix=\"AVAX-\"\n",
    "min_trades_in_set: int = 2\n",
    "week_floor=None"
   ],
   "id": "f849699dc8e4afa1",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:07:33.168860Z",
     "start_time": "2025-10-14T13:07:33.163299Z"
    }
   },
   "cell_type": "code",
   "source": "candidates",
   "id": "347f835f086445e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({0, 1}), 4)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:07:35.371877Z",
     "start_time": "2025-10-14T13:07:35.309517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Algorithm 2 (apply per candidate, never double-label)\n",
    "# start with unlabeled df; for each candidate, label its trades\n",
    "labeled_df = df_token.copy()\n",
    "# initialize label columns so .loc works even if no matches\n",
    "for col in [\"is_wash\",\"wash_id\",\"wash_window\",\"wash_window_start\",\"wash_window_end\",\n",
    "            \"wash_n_trades\",\"wash_m\",\"wash_v_mean\",\"wash_v_tol\",\"wash_max_abs_pos\"]:\n",
    "    if col not in labeled_df:\n",
    "        labeled_df[col] = np.nan if col not in {\"is_wash\"} else False\n",
    "\n",
    "for i, (S, _count) in enumerate(candidates):\n",
    "    print(f\"Processing {i+1} out of {len(candidates)} candidates\")\n",
    "    # run Algorithm 2 for this candidate set S\n",
    "    df_S_labeled = algorithm2_volume_matching(\n",
    "        labeled_df,\n",
    "        candidate_wallets=S,\n",
    "        m=m,\n",
    "        windows=windows,\n",
    "        min_trades_in_set=min_trades_in_set,\n",
    "        week_floor=week_floor,\n",
    "        id_prefix=id_prefix,\n",
    "    )\n",
    "\n",
    "    # merge new labels (logical OR for is_wash; overwrite diagnostics where newly labeled)\n",
    "    newly = df_S_labeled[\"is_wash\"] & ~labeled_df[\"is_wash\"]\n",
    "    for col in [\"is_wash\",\"wash_id\",\"wash_window\",\"wash_window_start\",\"wash_window_end\",\n",
    "                \"wash_n_trades\",\"wash_m\",\"wash_v_mean\",\"wash_v_tol\",\"wash_max_abs_pos\"]:\n",
    "        labeled_df.loc[newly, col] = df_S_labeled.loc[newly, col]"
   ],
   "id": "f8a21bcbdf6d702f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 out of 1 candidates\n",
      "Window 1h is in process\n",
      "Window 1D is in process\n",
      "Window 7D is in process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8223/1268690629.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['AVAX-1' 'AVAX-1' 'AVAX-1' 'AVAX-1' 'AVAX-1' 'AVAX-1' 'AVAX-1' 'AVAX-1'\n",
      " 'AVAX-1' 'AVAX-1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  labeled_df.loc[newly, col] = df_S_labeled.loc[newly, col]\n",
      "/tmp/ipykernel_8223/1268690629.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['1h' '1h' '1h' '1h' '1h' '1h' '1h' '1h' '1h' '1h']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  labeled_df.loc[newly, col] = df_S_labeled.loc[newly, col]\n",
      "/tmp/ipykernel_8223/1268690629.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<DatetimeArray>\n",
      "['1970-01-01 00:00:00', '1970-01-01 00:00:00', '1970-01-01 00:00:00',\n",
      " '1970-01-01 00:00:00', '1970-01-01 00:00:00', '1970-01-01 00:00:00',\n",
      " '1970-01-01 00:00:00', '1970-01-01 00:00:00', '1970-01-01 00:00:00',\n",
      " '1970-01-01 00:00:00']\n",
      "Length: 10, dtype: datetime64[ns]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  labeled_df.loc[newly, col] = df_S_labeled.loc[newly, col]\n",
      "/tmp/ipykernel_8223/1268690629.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<DatetimeArray>\n",
      "['1970-01-01 01:00:00', '1970-01-01 01:00:00', '1970-01-01 01:00:00',\n",
      " '1970-01-01 01:00:00', '1970-01-01 01:00:00', '1970-01-01 01:00:00',\n",
      " '1970-01-01 01:00:00', '1970-01-01 01:00:00', '1970-01-01 01:00:00',\n",
      " '1970-01-01 01:00:00']\n",
      "Length: 10, dtype: datetime64[ns]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  labeled_df.loc[newly, col] = df_S_labeled.loc[newly, col]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T13:07:46.576234Z",
     "start_time": "2025-10-14T13:07:46.568702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = summarize_wash_stats(labeled_df)\n",
    "print(candidates[:5])  # top candidate SCCs from Algorithm 1\n",
    "print(summary)         # quick overview for the token\n",
    "# labeled_df has per-trade flags/diagnostics: is_wash, wash_id, v_mean, v_tol, ..."
   ],
   "id": "123b5825d3b9a069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(frozenset({0, 1}), 4)]\n",
      "   n_trades_total  n_trades_wash  wash_trade_share_pct  volume_total  \\\n",
      "0              14             10             71.428571         140.0   \n",
      "\n",
      "   volume_wash  wash_volume_share_pct  n_wash_sets  \n",
      "0        100.0              71.428571            1  \n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T13:41:17.067188Z",
     "start_time": "2025-10-07T13:41:17.063600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = 1.1\n",
    "import sys\n",
    "sys.getsizeof(float(a))"
   ],
   "id": "d07683ea2c3f6d09",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T10:09:41.658159Z",
     "start_time": "2025-10-07T10:09:41.656131Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "76ef801660f85052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T14:11:20.708059Z",
     "start_time": "2025-10-07T14:10:54.075841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare_postmerge_csv.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "def make_txhash(row: pd.Series) -> str:\n",
    "    # stable pseudo-hash (not a blockchain txid): hex string\n",
    "    s = f\"{int(row['timestamp'])}|{row['seller']}|{row['buyer']}|{row['size']:.12g}\"\n",
    "    return \"tx-\" + hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def prepare_postmerge_csv(\n",
    "    df: pd.DataFrame,\n",
    "    out_csv: str,\n",
    "    token_id: str,\n",
    "    token_usd_const: float,\n",
    "    eth_usd_const: float | None = None,\n",
    "    price_is_token_in_eth: bool | None = None,\n",
    "    ether_address: str = \"0x0000000000000000000000000000000000000000\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build the 'post-merge' table the R pipeline expects, using constant prices.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    df: DataFrame with columns:\n",
    "        - price : float64   (optional; used only if price_is_token_in_eth=True)\n",
    "        - size  : float64   (token amount)\n",
    "        - time  : datetime64[ns] (tz-aware or tz-naive in UTC)\n",
    "        - seller: int/str\n",
    "        - buyer : int/str\n",
    "    token_id:     identifier for the (non-ETH) token (symbol or address)\n",
    "    token_usd_const: constant USD price for that token (applied to all rows)\n",
    "    eth_usd_const:   optional constant ETH/USD (only needed if you also want ETH amounts)\n",
    "    price_is_token_in_eth:\n",
    "        - If True: use df['price'] as token-in-ETH.\n",
    "        - If False/None: ignore df['price'] for ETH math; use eth_usd_const if provided.\n",
    "\n",
    "    Output CSV columns (matches R 'merge_*' output shape):\n",
    "        date, cut, blockNumber, timestamp, transactionHash,\n",
    "        eth_buyer, eth_seller, ether, token,\n",
    "        trade_amount_eth, trade_amount_dollar, trade_amount_token, token_price_in_eth\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- time handling (to UTC seconds) ---\n",
    "    # ensure datetime dtype\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    if df[\"time\"].dt.tz is None:\n",
    "        # assume already UTC if tz-naive\n",
    "        df[\"time\"] = df[\"time\"].dt.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        df[\"time\"] = df[\"time\"].dt.tz_convert(\"UTC\")\n",
    "    # integer seconds\n",
    "    df[\"timestamp\"] = (df[\"time\"].view(\"int64\") // 10**9).astype(\"int64\")\n",
    "\n",
    "    # start-of-day (UTC) seconds, used by the R code as 'cut'\n",
    "    df[\"cut\"] = (df[\"timestamp\"] // 86_400) * 86_400\n",
    "\n",
    "    # simple monotone blockNumber (only used for ordering in one place)\n",
    "    df[\"blockNumber\"] = df[\"timestamp\"].astype(\"int64\")\n",
    "\n",
    "    # transactionHash: make a deterministic synthetic id per row\n",
    "    df[\"transactionHash\"] = df.apply(make_txhash, axis=1)\n",
    "\n",
    "    # addresses as strings (R handles strings better than large ints)\n",
    "    df[\"eth_buyer\"] = df[\"seller\"].astype(str)   # note: we set like this so that when ether=FALSE the R code flips back\n",
    "    df[\"eth_seller\"] = df[\"buyer\"].astype(str)\n",
    "\n",
    "    # constant token id & ether address\n",
    "    df[\"token\"] = str(token_id)\n",
    "    df[\"ether\"] = ether_address\n",
    "\n",
    "    # trade amounts\n",
    "    df[\"trade_amount_token\"] = df[\"size\"].astype(float)\n",
    "\n",
    "    # token price in ETH (constant or from df['price'])\n",
    "    token_price_in_eth = np.nan\n",
    "    if price_is_token_in_eth is True and \"price\" in df.columns:\n",
    "        # take from the dataframe (row-specific). If you want a constant, overwrite below.\n",
    "        df[\"token_price_in_eth\"] = df[\"price\"].astype(float)\n",
    "    elif (eth_usd_const is not None) and (token_usd_const is not None):\n",
    "        # constant derived from USDs\n",
    "        token_price_in_eth = float(token_usd_const) / float(eth_usd_const)\n",
    "        df[\"token_price_in_eth\"] = token_price_in_eth\n",
    "    else:\n",
    "        df[\"token_price_in_eth\"] = np.nan  # optional field\n",
    "\n",
    "    # trade_amount_eth (only useful if you plan to run ether=TRUE in R; harmless otherwise)\n",
    "    if \"token_price_in_eth\" in df.columns and df[\"token_price_in_eth\"].notna().any():\n",
    "        df[\"trade_amount_eth\"] = df[\"trade_amount_token\"] * df[\"token_price_in_eth\"]\n",
    "    else:\n",
    "        df[\"trade_amount_eth\"] = 0.0\n",
    "\n",
    "    # trade_amount_dollar using constant token USD price (your chosen quick method)\n",
    "    df[\"trade_amount_dollar\"] = df[\"trade_amount_token\"] * float(token_usd_const)\n",
    "\n",
    "    # nice 'date' column for summaries (UTC date)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"cut\"], unit=\"s\", utc=True).dt.date\n",
    "\n",
    "    # final column order (mirrors R)\n",
    "    cols = [\n",
    "        \"date\",\n",
    "        \"cut\",\n",
    "        \"blockNumber\",\n",
    "        \"timestamp\",\n",
    "        \"transactionHash\",\n",
    "        \"eth_buyer\",\n",
    "        \"eth_seller\",\n",
    "        \"ether\",\n",
    "        \"token\",\n",
    "        \"trade_amount_eth\",\n",
    "        \"trade_amount_dollar\",\n",
    "        \"trade_amount_token\",\n",
    "        \"token_price_in_eth\",\n",
    "    ]\n",
    "    out = df[cols].sort_values([\"token\", \"blockNumber\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    # write CSV\n",
    "    Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved prepared post-merge trades to: {out_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prepare_postmerge_csv(\n",
    "    df_avax,\n",
    "    out_csv=\"prepared_trades.csv\",\n",
    "    token_id=\"AVAX\",\n",
    "    token_usd_const=30,     # constant token USD price (your quick method)\n",
    "    eth_usd_const=3500.0,     # optional: to also compute token_price_in_eth & trade_amount_eth\n",
    "    price_is_token_in_eth=False,  # set True only if you want to take per-row df['price'] as token-in-ETH\n",
    ")\n"
   ],
   "id": "e481bd96d8182136",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13799/4057097084.py:55: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[\"timestamp\"] = (df[\"time\"].view(\"int64\") // 10**9).astype(\"int64\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prepared post-merge trades to: prepared_trades.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T08:36:36.092787Z",
     "start_time": "2025-10-08T08:36:36.085873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = 1\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.getsizeof(np.int32(a))"
   ],
   "id": "159d5394a7a046b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T08:34:50.646739Z",
     "start_time": "2025-10-08T08:34:50.639799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = 256\n",
    "b = 256\n",
    "a is b  # True (interned small int)"
   ],
   "id": "f0f521425176bdd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T08:34:51.071884Z",
     "start_time": "2025-10-08T08:34:51.069280Z"
    }
   },
   "cell_type": "code",
   "source": "id(a)",
   "id": "20265ba88a6a9736",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9675024"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T08:34:52.760314Z",
     "start_time": "2025-10-08T08:34:52.755969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "a = 257\n",
    "b = 257\n",
    "a is b  # False (different objects)\n"
   ],
   "id": "f5c1f0f6041c8a51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T08:34:54.487578Z",
     "start_time": "2025-10-08T08:34:54.482953Z"
    }
   },
   "cell_type": "code",
   "source": "id(a)",
   "id": "13a8200173d26779",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136198989375216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "efe6d152bf92efe6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

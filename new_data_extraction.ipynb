{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T12:20:52.752263Z",
     "start_time": "2025-10-16T12:20:52.482693Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d208151c09e63b46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T12:20:53.965721Z",
     "start_time": "2025-10-16T12:20:53.650297Z"
    }
   },
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "HOME_DIR = os.path.dirname(BASE_DIR)\n",
    "new_data_dates = os.listdir(os.path.join(HOME_DIR, \"hl-node-fills\"))\n",
    "\n",
    "# --- Config/paths ---\n",
    "DATA_DIR = Path(os.path.join(HOME_DIR, \"data\"))\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WALLETS_CSV = DATA_DIR / \"wallet_db.csv\""
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/asevlad/program_files/github_asevlad/hl-node-fills'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m BASE_DIR \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetcwd()\n\u001B[1;32m      2\u001B[0m HOME_DIR \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(BASE_DIR)\n\u001B[0;32m----> 3\u001B[0m new_data_dates \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(HOME_DIR, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhl-node-fills\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# --- Config/paths ---\u001B[39;00m\n\u001B[1;32m      6\u001B[0m DATA_DIR \u001B[38;5;241m=\u001B[39m Path(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(HOME_DIR, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/asevlad/program_files/github_asevlad/hl-node-fills'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f544fa9b-aa1f-40bd-8f48-11a89c0f7d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Wallet DB helpers ---\n",
    "def load_wallet_db(csv_path: Path = WALLETS_CSV) -> Tuple[Dict[str, int], int]:\n",
    "    \"\"\"\n",
    "    Load wallets from CSV into a dict {wallet: wallet_id}, return dict and next_id.\n",
    "    If file doesn't exist, start fresh at 1.\n",
    "    \"\"\"\n",
    "    mapping: Dict[str, int] = {}\n",
    "    next_id = 1\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path, dtype={\"wallet_id\": \"uint32\", \"wallet\": \"string\"})\n",
    "        if not df.empty:\n",
    "            for wid, wal in zip(df[\"wallet_id\"].astype(\"uint32\"), df[\"wallet\"].astype(\"string\")):\n",
    "                mapping[str(wal)] = int(wid)\n",
    "            next_id = int(df[\"wallet_id\"].max()) + 1\n",
    "    else:\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        pd.DataFrame(columns=[\"wallet_id\", \"wallet\"]).to_csv(csv_path, index=False)\n",
    "    return mapping, next_id\n",
    "\n",
    "def append_wallet(csv_path: Path, wallet: str, wallet_id: int) -> None:\n",
    "    \"\"\"Append a single wallet row to the CSV.\"\"\"\n",
    "    pd.DataFrame([{\"wallet_id\": wallet_id, \"wallet\": wallet}]).to_csv(\n",
    "        csv_path, mode=\"a\", header=False, index=False\n",
    "    )\n",
    "\n",
    "def get_wallet_id(wallet: str, mapping: Dict[str, int], next_id_ref: List[int], csv_path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Return wallet_id for wallet, creating a new id if needed.\n",
    "    next_id_ref is a single-item list to allow in-place increment.\n",
    "    \"\"\"\n",
    "    w = str(wallet)\n",
    "    wid = mapping.get(w)\n",
    "    if wid is not None:\n",
    "        return wid\n",
    "    wid = next_id_ref[0]\n",
    "    mapping[w] = wid\n",
    "    next_id_ref[0] += 1\n",
    "    append_wallet(csv_path, w, wid)\n",
    "    return wid\n",
    "\n",
    "def retrieve_data(file_path: Path, wallet_map: Dict[str, int], next_id_ref: List[int], wallets_csv: Path = WALLETS_CSV) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a newline-delimited JSON file of trades and produce a normalized DataFrame\n",
    "    for later partitioned saving.\n",
    "    Output columns: coin, price, size, time, is_ask, wallet_id\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    with open(file_path) as f:\n",
    "        append = records.append\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            wallet, trade = json.loads(line)\n",
    "            if int(trade.get(\"tid\")) != 0:\n",
    "                wallet_id = get_wallet_id(wallet, wallet_map, next_id_ref, wallets_csv)\n",
    "        \n",
    "                px = trade.get(\"px\")\n",
    "                sz = trade.get(\"sz\")\n",
    "                # skip malformed\n",
    "                if px is None or sz is None:\n",
    "                    continue\n",
    "        \n",
    "                append(\n",
    "                    {\n",
    "                        \"coin\": trade.get(\"coin\"),\n",
    "                        \"price\": float(px),\n",
    "                        \"size\": float(sz),\n",
    "                        \"time\": trade.get(\"time\"),\n",
    "                        \"is_ask\": trade.get(\"side\") == \"A\",\n",
    "                        \"wallet_id\": wallet_id,\n",
    "                        \"tid\": trade.get(\"tid\"),\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Types & cleaning\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\", unit=\"ms\")\n",
    "    df = df.dropna(subset=[\"time\"])\n",
    "    # enforce dtypes\n",
    "    df[\"price\"] = df[\"price\"].astype(\"float32\")\n",
    "    df[\"size\"] = df[\"size\"].astype(\"float32\")\n",
    "    df[\"is_ask\"] = df[\"is_ask\"].astype(\"bool\")\n",
    "    df[\"wallet_id\"] = df[\"wallet_id\"].astype(\"uint32\")\n",
    "    df[\"tid\"] = df[\"tid\"].astype(\"uint32\")\n",
    "    \n",
    "    return df[[\"coin\", \"price\", \"size\", \"time\", \"is_ask\", \"wallet_id\", \"tid\"]]\n",
    "\n",
    "def convert_to_trade_df(df):\n",
    "    df_ask = df[df[\"is_ask\"]].drop(\"is_ask\", axis=1)\n",
    "    df_bid = df[~df[\"is_ask\"]].drop(\"is_ask\", axis=1)\n",
    "    df_trades = pd.merge(\n",
    "        df_ask,\n",
    "        df_bid,\n",
    "        on=[\"coin\", \"price\", \"size\", \"time\", \"tid\"],\n",
    "        how=\"outer\",\n",
    "        validate=\"one_to_one\",\n",
    "        suffixes=(\"_ask\",\"_bid\"),\n",
    "    ).rename(columns={\"wallet_id_ask\":\"seller\", \"wallet_id_bid\":\"buyer\"}).drop(\"tid\", axis=1)\n",
    "    return df_trades\n",
    "\n",
    "def _target_path_for(coin: str, dt: pd.Timestamp) -> Path:\n",
    "    return DATA_DIR / str(coin) / f\"{dt.date()}.parquet\"\n",
    "\n",
    "def _write_daily_parquet(target: Path, df_day: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Write/merge the daily file. If target exists, read, concat, de-dup, sort, write.\n",
    "    We de-dup on [time, wallet_id, price, size, is_ask] as a reasonable row identity.\n",
    "    \"\"\"\n",
    "    target.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Keep only required columns & types\n",
    "    cols = [\"price\", \"size\", \"time\", \"seller\", \"buyer\"]\n",
    "    df_day = df_day[cols].copy()\n",
    "\n",
    "    if target.exists():\n",
    "        try:\n",
    "            old = pd.read_parquet(target, engine=\"pyarrow\")\n",
    "            # Cast to same dtypes to avoid upcasting surprises\n",
    "            old[\"price\"] = old[\"price\"].astype(\"float32\")\n",
    "            old[\"size\"] = old[\"size\"].astype(\"float32\")\n",
    "            old[\"time\"] = pd.to_datetime(old[\"time\"], errors=\"coerce\")\n",
    "            old[\"seller\"] = old[\"seller\"].astype(\"uint64\")\n",
    "            old[\"buyer\"] = old[\"buyer\"].astype(\"uint64\")\n",
    "            df_day = pd.concat([old, df_day], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read existing parquet {target}: {e}. Overwriting.\")\n",
    "\n",
    "    df_day = df_day.dropna(subset=[\"time\"]).drop_duplicates(\n",
    "        subset=[\"time\", \"seller\", \"buyer\", \"price\", \"size\"], keep=\"last\"\n",
    "    )\n",
    "    df_day = df_day.sort_values(\"time\")\n",
    "    df_day.to_parquet(target, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "def save_partitioned(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Save rows to data/<coin>/<YYYY-MM-DD>.parquet, merging per-day files if present.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    # Add date for grouping\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = df[\"time\"].dt.date\n",
    "\n",
    "    # Group by coin/date\n",
    "    for (coin, day), g in df.groupby([\"coin\", \"date\"], sort=False):\n",
    "        if pd.isna(coin) or coin == \"\":\n",
    "            logger.warning(\"Skipping rows with empty coin.\")\n",
    "            continue\n",
    "        target = DATA_DIR / str(coin) / f\"{day}.parquet\"\n",
    "        _write_daily_parquet(target, g)\n",
    "\n",
    "    logger.info(\"Data has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "0c5f03ea-a379-44c5-a344-86b91c765624",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T12:20:54.490369Z",
     "start_time": "2025-10-16T12:20:54.474634Z"
    }
   },
   "source": [
    "new_data_folders = os.listdir(os.path.join(HOME_DIR, \"hl-node-fills\"))\n",
    "\n",
    "wallet_map, next_id = load_wallet_db()\n",
    "next_id_ref = [next_id]  # mutable holder\n",
    "\n",
    "\n",
    "for i, date in enumerate(new_data_folders):\n",
    "    hour_file_names = os.listdir(os.path.join(HOME_DIR, \"hl-node-fills\", date))\n",
    "\n",
    "    for file_name in hour_file_names:\n",
    "        file_full_path = os.path.join(HOME_DIR, \"hl-node-fills\", date, file_name)\n",
    "\n",
    "        logger.info(f\"{file_full_path} is processing\")\n",
    "        df = retrieve_data(Path(file_full_path), wallet_map, next_id_ref, WALLETS_CSV)\n",
    "        trade_df = convert_to_trade_df(df)\n",
    "        save_partitioned(trade_df)\n",
    "\n",
    "    logger.info(f\"Processed {i} out of {len(new_data_folders)}\")"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/asevlad/program_files/github_asevlad/hl-node-fills'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m new_data_folders \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(HOME_DIR, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhl-node-fills\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      3\u001B[0m wallet_map, next_id \u001B[38;5;241m=\u001B[39m load_wallet_db()\n\u001B[1;32m      4\u001B[0m next_id_ref \u001B[38;5;241m=\u001B[39m [next_id]  \u001B[38;5;66;03m# mutable holder\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/asevlad/program_files/github_asevlad/hl-node-fills'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030f901-b94f-4c83-9c18-78009856fc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca28208-3fe4-4a84-8353-30c1f093f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f9fabc7746c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.574135Z",
     "start_time": "2025-08-07T14:28:14.570969Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8426136bce776733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.630828Z",
     "start_time": "2025-08-07T14:28:14.627980Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "new_data_folders = os.listdir(os.path.join(HOME_DIR, \"hl-node-fills\"))\n",
    "\n",
    "wallet_map, next_id = load_wallet_db()\n",
    "next_id_ref = [next_id]  # mutable holder\n",
    "file_full_path = os.path.join(HOME_DIR, \"hl-node-fills\", \"20250716\", \"0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83efbd5b7813f05d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.734486Z",
     "start_time": "2025-08-07T14:28:14.731614Z"
    }
   },
   "outputs": [],
   "source": [
    "# logger.info(f\"{file_full_path} is processing\")\n",
    "# df = retrieve_data(Path(file_full_path), wallet_map, next_id_ref, WALLETS_CSV)\n",
    "trade_df = convert_to_trade_df(df)\n",
    "# save_partitioned(trade_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de2fa8b7b8fe2ffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.785636Z",
     "start_time": "2025-08-07T14:28:14.783052Z"
    }
   },
   "outputs": [],
   "source": [
    "dfg = df.groupby([\"coin\", \"price\", \"size\", \"time\", \"tid\"]).size().reset_index(name=\"n_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68d0efafb9908d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.890020Z",
     "start_time": "2025-08-07T14:28:14.887339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coin</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>time</th>\n",
       "      <th>is_ask</th>\n",
       "      <th>wallet_id</th>\n",
       "      <th>tid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [coin, price, size, time, is_ask, wallet_id, tid]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"tid\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2af844e7bc9b3c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.942641Z",
     "start_time": "2025-08-07T14:28:14.939568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_df[(trade_df[\"seller\"].isna()) & (trade_df[\"buyer\"].isna())].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d25f00cd37feb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.994712Z",
     "start_time": "2025-08-07T14:28:14.991613Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e630a176d93e21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:15.046308Z",
     "start_time": "2025-08-07T14:28:15.043603Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf3a146d858753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

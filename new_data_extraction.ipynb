{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:19.400877Z",
     "start_time": "2025-08-07T14:28:19.397855Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d208151c09e63b46",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "HOME_DIR = os.path.dirname(BASE_DIR)\n",
    "new_data_dates = os.listdir(os.path.join(HOME_DIR, \"hl-node-fills\"))\n",
    "\n",
    "# --- Config/paths ---\n",
    "DATA_DIR = Path(os.path.join(HOME_DIR, \"data\"))\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WALLETS_CSV = DATA_DIR / \"wallet_db.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca381a-04d8-402b-8ce6-c1565c45b20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d86c65-cdc7-43f6-a010-0403492e26ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65b774-113d-439a-a4e8-3d14834019cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f544fa9b-aa1f-40bd-8f48-11a89c0f7d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Wallet DB helpers ---\n",
    "def load_wallet_db(csv_path: Path = WALLETS_CSV) -> Tuple[Dict[str, int], int]:\n",
    "    \"\"\"\n",
    "    Load wallets from CSV into a dict {wallet: wallet_id}, return dict and next_id.\n",
    "    If file doesn't exist, start fresh at 1.\n",
    "    \"\"\"\n",
    "    mapping: Dict[str, int] = {}\n",
    "    next_id = 1\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path, dtype={\"wallet_id\": \"uint32\", \"wallet\": \"string\"})\n",
    "        if not df.empty:\n",
    "            for wid, wal in zip(df[\"wallet_id\"].astype(\"uint32\"), df[\"wallet\"].astype(\"string\")):\n",
    "                mapping[str(wal)] = int(wid)\n",
    "            next_id = int(df[\"wallet_id\"].max()) + 1\n",
    "    else:\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        pd.DataFrame(columns=[\"wallet_id\", \"wallet\"]).to_csv(csv_path, index=False)\n",
    "    return mapping, next_id\n",
    "\n",
    "def append_wallet(csv_path: Path, wallet: str, wallet_id: int) -> None:\n",
    "    \"\"\"Append a single wallet row to the CSV.\"\"\"\n",
    "    pd.DataFrame([{\"wallet_id\": wallet_id, \"wallet\": wallet}]).to_csv(\n",
    "        csv_path, mode=\"a\", header=False, index=False\n",
    "    )\n",
    "\n",
    "def get_wallet_id(wallet: str, mapping: Dict[str, int], next_id_ref: List[int], csv_path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Return wallet_id for wallet, creating a new id if needed.\n",
    "    next_id_ref is a single-item list to allow in-place increment.\n",
    "    \"\"\"\n",
    "    w = str(wallet)\n",
    "    wid = mapping.get(w)\n",
    "    if wid is not None:\n",
    "        return wid\n",
    "    wid = next_id_ref[0]\n",
    "    mapping[w] = wid\n",
    "    next_id_ref[0] += 1\n",
    "    append_wallet(csv_path, w, wid)\n",
    "    return wid\n",
    "\n",
    "# --- ETL helpers ---\n",
    "def _infer_is_ask(trade: dict, idx_in_side_info: int) -> bool:\n",
    "    \"\"\"\n",
    "    Decide if the row belongs to the ask side.\n",
    "    Heuristic:\n",
    "      1) If top-level 'side' is present ('A'/'B'), we tag A as asks, B as bids.\n",
    "      2) Otherwise fall back to index parity inside side_info: even->A(ask), odd->B(bid).\n",
    "    Adjust here if your ground truth differs.\n",
    "    \"\"\"\n",
    "    side_top = trade.get(\"side\")\n",
    "    if side_top in (\"A\", \"B\"):\n",
    "        # We still need to label both rows A/B for the two entriesâ€”we'll mirror top-level:\n",
    "        is_ask = (side_top == \"A\")\n",
    "    else:\n",
    "        is_ask = (idx_in_side_info % 2 == 0)  # even -> \"A\" (ask), odd -> \"B\" (bid)\n",
    "    return bool(is_ask)\n",
    "\n",
    "def retrieve_data(file_path: Path, wallet_map: Dict[str, int], next_id_ref: List[int], wallets_csv: Path = WALLETS_CSV) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a newline-delimited JSON file of trades and produce a normalized DataFrame\n",
    "    for later partitioned saving.\n",
    "    Output columns: coin, price, size, time, is_ask, wallet_id\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    with open(file_path) as f:\n",
    "        append = records.append\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            wallet, trade = json.loads(line)\n",
    "            wallet_id = get_wallet_id(wallet, wallet_map, next_id_ref, wallets_csv)\n",
    "    \n",
    "            px = trade.get(\"px\")\n",
    "            sz = trade.get(\"sz\")\n",
    "            # skip malformed\n",
    "            if px is None or sz is None:\n",
    "                continue\n",
    "    \n",
    "            append(\n",
    "                {\n",
    "                    \"coin\": trade.get(\"coin\"),\n",
    "                    \"price\": float(px),\n",
    "                    \"size\": float(sz),\n",
    "                    \"time\": trade.get(\"time\"),\n",
    "                    \"is_ask\": trade.get(\"side\") == \"A\",\n",
    "                    \"wallet_id\": wallet_id,\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Types & cleaning\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\", unit=\"ms\")\n",
    "    df = df.dropna(subset=[\"time\"])\n",
    "    # enforce dtypes\n",
    "    df[\"price\"] = df[\"price\"].astype(\"float32\")\n",
    "    df[\"size\"] = df[\"size\"].astype(\"float32\")\n",
    "    df[\"is_ask\"] = df[\"is_ask\"].astype(\"bool\")\n",
    "    df[\"wallet_id\"] = df[\"wallet_id\"].astype(\"uint32\")\n",
    "    \n",
    "    return df[[\"coin\", \"price\", \"size\", \"time\", \"is_ask\", \"wallet_id\"]]\n",
    "\n",
    "def _target_path_for(coin: str, dt: pd.Timestamp) -> Path:\n",
    "    return DATA_DIR / str(coin) / f\"{dt.date()}.parquet\"\n",
    "\n",
    "def _write_daily_parquet(target: Path, df_day: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Write/merge the daily file. If target exists, read, concat, de-dup, sort, write.\n",
    "    We de-dup on [time, wallet_id, price, size, is_ask] as a reasonable row identity.\n",
    "    \"\"\"\n",
    "    target.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Keep only required columns & types\n",
    "    cols = [\"price\", \"size\", \"time\", \"is_ask\", \"wallet_id\"]\n",
    "    df_day = df_day[cols].copy()\n",
    "\n",
    "    if target.exists():\n",
    "        try:\n",
    "            old = pd.read_parquet(target, engine=\"pyarrow\")\n",
    "            # Cast to same dtypes to avoid upcasting surprises\n",
    "            old[\"price\"] = old[\"price\"].astype(\"float32\")\n",
    "            old[\"size\"] = old[\"size\"].astype(\"float32\")\n",
    "            old[\"time\"] = pd.to_datetime(old[\"time\"], errors=\"coerce\")\n",
    "            old[\"is_ask\"] = old[\"is_ask\"].astype(\"bool\")\n",
    "            old[\"wallet_id\"] = old[\"wallet_id\"].astype(\"uint32\")\n",
    "            df_day = pd.concat([old, df_day], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read existing parquet {target}: {e}. Overwriting.\")\n",
    "\n",
    "    df_day = df_day.dropna(subset=[\"time\"]).drop_duplicates(\n",
    "        subset=[\"time\", \"wallet_id\", \"price\", \"size\", \"is_ask\"], keep=\"last\"\n",
    "    )\n",
    "    df_day = df_day.sort_values(\"time\")\n",
    "    df_day.to_parquet(target, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "def save_partitioned(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Save rows to data/<coin>/<YYYY-MM-DD>.parquet, merging per-day files if present.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    # Add date for grouping\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = df[\"time\"].dt.date\n",
    "\n",
    "    # Group by coin/date\n",
    "    for (coin, day), g in df.groupby([\"coin\", \"date\"], sort=False):\n",
    "        if pd.isna(coin) or coin == \"\":\n",
    "            logger.warning(\"Skipping rows with empty coin.\")\n",
    "            continue\n",
    "        target = DATA_DIR / str(coin) / f\"{day}.parquet\"\n",
    "        _write_daily_parquet(target, g)\n",
    "\n",
    "    logger.info(\"Data has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f03ea-a379-44c5-a344-86b91c765624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-20 17:34:32.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m/home/debian/hl-node-fills/20250617/10.json is processing\u001b[0m\n",
      "\u001b[32m2025-08-20 17:34:36.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_partitioned\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mData has been saved successfully.\u001b[0m\n",
      "\u001b[32m2025-08-20 17:34:36.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m/home/debian/hl-node-fills/20250617/14.json is processing\u001b[0m\n",
      "\u001b[32m2025-08-20 17:34:41.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_partitioned\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mData has been saved successfully.\u001b[0m\n",
      "\u001b[32m2025-08-20 17:34:42.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m/home/debian/hl-node-fills/20250617/12.json is processing\u001b[0m\n",
      "\u001b[32m2025-08-20 17:34:46.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_partitioned\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mData has been saved successfully.\u001b[0m\n",
      "\u001b[32m2025-08-20 17:34:46.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m/home/debian/hl-node-fills/20250617/19.json is processing\u001b[0m\n",
      "\u001b[32m2025-08-20 17:34:53.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_partitioned\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mData has been saved successfully.\u001b[0m\n",
      "\u001b[32m2025-08-20 17:34:53.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m/home/debian/hl-node-fills/20250617/0.json is processing\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "new_data_folders = os.listdir(os.path.join(HOME_DIR, \"hl-node-fills\"))\n",
    "\n",
    "wallet_map, next_id = load_wallet_db()\n",
    "next_id_ref = [next_id]  # mutable holder\n",
    "\n",
    "\n",
    "for i, date in enumerate(new_data_folders):\n",
    "    hour_file_names = os.listdir(os.path.join(HOME_DIR, \"hl-node-fills\", date))\n",
    "\n",
    "    for file_name in hour_file_names:\n",
    "        file_full_path = os.path.join(HOME_DIR, \"hl-node-fills\", date, file_name)\n",
    "\n",
    "        logger.info(f\"{file_full_path} is processing\")\n",
    "        df = retrieve_data(Path(file_full_path), wallet_map, next_id_ref, WALLETS_CSV)\n",
    "        save_partitioned(df)\n",
    "\n",
    "    logger.info(f\"Processed {i} out of {len(new_data_folders)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030f901-b94f-4c83-9c18-78009856fc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca28208-3fe4-4a84-8353-30c1f093f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f9fabc7746c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.574135Z",
     "start_time": "2025-08-07T14:28:14.570969Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426136bce776733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.630828Z",
     "start_time": "2025-08-07T14:28:14.627980Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f2d2a9cd7d93b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.682672Z",
     "start_time": "2025-08-07T14:28:14.679552Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efbd5b7813f05d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.734486Z",
     "start_time": "2025-08-07T14:28:14.731614Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2fa8b7b8fe2ffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.785636Z",
     "start_time": "2025-08-07T14:28:14.783052Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e594574217264c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.837570Z",
     "start_time": "2025-08-07T14:28:14.834629Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0efafb9908d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.890020Z",
     "start_time": "2025-08-07T14:28:14.887339Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af844e7bc9b3c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.942641Z",
     "start_time": "2025-08-07T14:28:14.939568Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d25f00cd37feb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:14.994712Z",
     "start_time": "2025-08-07T14:28:14.991613Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e630a176d93e21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:28:15.046308Z",
     "start_time": "2025-08-07T14:28:15.043603Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf3a146d858753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
